# Celebal_Summer-Internship_2025
Week 1 (Python Basics)
- What is python
- Installing python
- Basic syntax
- Lists
- Tuples
- Dictinories
- Sets
- Strings
- Condtional Statement
- loops
- creating modules


Week 2 
## üìö Contents

1. **Core OOP Concepts**
   - Classes and Objects
   - Inheritance
   - Polymorphism
   - Encapsulation
   - special method
   
2. - pip
   - flask web framework
   - Django web framework

Week 3 
1. Learn python libaries
   - Numpy
   - Pandas
   - Matplotlib
2. what is machine learning
   -Matrices, Vectors, Matrix multiplication, Transpose, Determinant, Eigenvalues and eigenvectors, Singular value decomposition
   
4. Data Cleaning, Data Transformation, Data Normalization, Data Scaling, Data Encoding, Data Imputation, Handling Missing Data, Data Reduction, Data Integration, Data Sampling

Week 4 
üìä - Feature Engineering & Dimensionality Reduction

- Feature Selection: Choosing relevant features.
- Feature Extraction: Creating new features from raw data.
- Feature Encoding: Converting categorical data to numerical (e.g., One-Hot, Label Encoding).
- Dimensionality Reduction: Reducing feature space (PCA, LDA, t-SNE, Autoencoders).

- PCA: Principal Component Analysis for linear dimensionality reduction.
- LDA: Linear Discriminant Analysis for supervised dimensionality reduction.
- t-SNE: t-Distributed Stochastic Neighbor Embedding for visualization.
- Autoencoders: Neural networks for nonlinear dimensionality reduction.

‚öôÔ∏è Model Training & Evaluation
- Train-Test Split: Dividing data into training and testing sets.
- Cross-Validation: K-Fold validation for robust model evaluation.
- Hyperparameter Tuning: Optimizing model parameters.
- Grid Search: Exhaustive hyperparameter search.
- Random Search: Randomized hyperparameter optimization.

üìà Performance Metrics
- Confusion Matrix: TP, TN, FP, FN visualization.
- Accuracy: (TP + TN) / Total predictions.
- Precision: TP / (TP + FP).
- Recall (Sensitivity): TP / (TP + FN).
- F1-Score: Harmonic mean of precision and recall.
- ROC Curve: Visualizing classifier performance.
- AUC Score: Area Under ROC Curve.

Week 5 

üìà Regression Models
- Simple Linear Regression: Single predictor, linear relationship.
- Multiple Linear Regression: Multiple predictors, linear relationship.
- Polynomial Regression: Non-linear relationships via polynomial features.
- Ridge Regression (L2): Prevents overfitting with L2 regularization.
- Lasso Regression (L1): Feature selection via L1 regularization.
- ElasticNet Regression: Combines L1 & L2 penalties (Ridge + Lasso).

üîç Classification Models
- Logistic Regression: Binary/multi-class classification (sigmoid function).
- Naive Bayes: Probabilistic classifier based on Bayes‚Äô theorem.
- k-Nearest Neighbors (k-NN): Instance-based learning (distance metrics).
- Decision Trees: Rule-based splits (entropy/Gini impurity).
- Random Forest: Ensemble of decision trees (bagging + feature randomness).
- Support Vector Machines (SVM): Maximizes margin using kernels (linear, RBF).
- 

 Week 6: Clustering & Ensemble Methods

### üîç Clustering
- **K-Means**: Elbow method + silhouette analysis
- **Hierarchical**: Dendrograms & linkage methods
- **DBSCAN**: Density-based clustering with noise handling
- **GMM**: Probabilistic clustering with EM algorithm

### üå≥ Ensemble Methods
- **Bagging**: Random Forest implementations
- **Boosting**: XGBoost/AdaBoost comparisons
- **Stacking**: Meta-model performance analysis

### üõ°Ô∏è Regularization
- L1 (Lasso), L2 (Ridge), ElasticNet implementations
- Feature selection visualizations






